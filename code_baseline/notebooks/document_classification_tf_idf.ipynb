{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:99% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "# make the Jupyter notebook use the full screen width\n",
    "display(HTML(\"<style>.container { width:99% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n",
      "  import imp\r\n"
     ]
    }
   ],
   "source": [
    "!python ../src/get_data_newsgroups_binary.py \\\n",
    "--output_dir /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "857 documents - 1.875MB (training set)\n",
      "2 categories\n",
      "\n",
      "Using Tfidf Vectorizer.\n",
      "Extracting features via sklearn.feature_selection.SelectKbest using LinearSVC.\n"
     ]
    }
   ],
   "source": [
    "!python ../src/train.py \\\n",
    "--filename /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup_binary/train_data.tsv \\\n",
    "--output_dir  /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/models  \\\n",
    "--vectorizer_type tfidf \\\n",
    "--feature_selection_svc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "570 documents - 1.299MB (test set)\n",
      "2 categories\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       319\n",
      "           1       0.77      0.80      0.79       251\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       570\n",
      "   macro avg       0.81      0.81      0.81       570\n",
      "weighted avg       0.81      0.81      0.81       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../src/test.py \\\n",
    "--filename /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup_binary/test_data.tsv  \\\n",
    "--model_path  /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/models/model.p \\\n",
    "--output_file /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict.py works on plain text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 documents - 1.299MB (test set)\n",
      "\n",
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "!python ../src/predict.py \\\n",
    "--filename /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup_binary/test_data_base64 \\\n",
    "--model_path  /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/models/model.p \\\n",
    "--output_file /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_binary_testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "test_labels=open( os.path.join(  \"/notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_binary_testing\"  )  ).read().rstrip(\"\\n\").split(\"\\n\" )\n",
    "test_labels=[ int(label) for label in test_labels ]\n",
    "\n",
    "pred_labels=open( os.path.join( '/notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_binary_testing'  )  ).read().rstrip(\"\\n\").split(\"\\n\")\n",
    "pred_labels=[ int(label.split()[0] ) for label in pred_labels ]\n",
    "\n",
    "pred_labels=open( os.path.join( '/notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_binary'  )  ).read().rstrip(\"\\n\").split(\"\\n\")\n",
    "pred_labels=[ int(label.split()[0]) for label in pred_labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8087719298245614\n",
      "0.7730769230769231\n",
      "0.8007968127490039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "print(accuracy_score(test_labels, pred_labels ))\n",
    "print(precision_score( test_labels, pred_labels ))\n",
    "print(recall_score( test_labels, pred_labels ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "#!python ../code_baseline/src/inference.py \\\n",
    "#--model_path  /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/output_folder_binary/model.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n",
      "  import imp\r\n"
     ]
    }
   ],
   "source": [
    "!python ../src/get_data_newsgroups.py \\\n",
    "--output_dir /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2034 documents - 3.980MB (training set)\n",
      "4 categories\n",
      "\n",
      "Using Tfidf Vectorizer.\n",
      "Extracting features via sklearn.feature_selection.SelectKbest using LinearSVC.\n"
     ]
    }
   ],
   "source": [
    "!python ../src/train.py \\\n",
    "--filename /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup/train_data.tsv \\\n",
    "--output_dir  /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/models_multiclass  \\\n",
    "--vectorizer_type tfidf \\\n",
    "--feature_selection_svc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82       319\n",
      "           1       0.92      0.95      0.93       389\n",
      "           2       0.92      0.95      0.93       394\n",
      "           3       0.79      0.75      0.77       251\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1353\n",
      "   macro avg       0.87      0.86      0.86      1353\n",
      "weighted avg       0.87      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../src/test.py \\\n",
    "--filename /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup/test_data.tsv  \\\n",
    "--model_path  /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/models_multiclass/model.p \\\n",
    "--output_file /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1353 documents - 2.867MB (test set)\n",
      "\n",
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "!python ../src/predict.py \\\n",
    "--filename /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup/test_data_base64 \\\n",
    "--model_path  /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/models_multiclass/model.p \\\n",
    "--output_file /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_multiclass_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification on IMDB test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first download 'IMDB Dataset.csv' from https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/data, and put it in the input directory\n",
    "!python ../src/get_data_imdb.py \\\n",
    "--input_dir /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/imdb  \\\n",
    "--output_dir /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/imdb #\\\n",
    "#--shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "25000 documents - 32.700MB (training set)\n",
      "2 categories\n",
      "\n",
      "Using Tfidf Vectorizer.\n",
      "Extracting features via sklearn.feature_selection.SelectKbest using LinearSVC.\n"
     ]
    }
   ],
   "source": [
    "!python ../src/train.py \\\n",
    "--filename /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/imdb/train_data.tsv \\\n",
    "--output_dir  /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/models_imdb  \\\n",
    "--vectorizer_type tfidf \\\n",
    "--feature_selection_svc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "25000 documents - 32.784MB (test set)\n",
      "2 categories\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88     12526\n",
      "           1       0.88      0.89      0.88     12474\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../src/test.py \\\n",
    "--filename /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/imdb/test_data.tsv  \\\n",
    "--model_path  /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/models_imdb/model.p \\\n",
    "--output_file /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 documents - 32.784MB (test set)\n",
      "\n",
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "!python ../src/predict.py \\\n",
    "--filename /notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/imdb/test_data_base64 \\\n",
    "--model_path  /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/models_imdb/model.p \\\n",
    "--output_file /notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_imdb_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some sanity checks:\n",
    "\n",
    "import os\n",
    "test_labels=open( os.path.join( '/notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/imdb/test_labels'  )  ).read().rstrip(\"\\n\").split(\"\\n\" )\n",
    "test_labels=[ int(label) for label in test_labels ]\n",
    "\n",
    "pred_labels=open( os.path.join( '/notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_imdb'  )  ).read().rstrip(\"\\n\").split(\"\\n\")\n",
    "pred_labels=[ int(label.split()[0] ) for label in pred_labels ]\n",
    "\n",
    "pred_labels=open( os.path.join( '/notebook/nas-trainings/arne/DGFISMA/Doc_class-docker/code_baseline/notebooks/results/results_tf_idf_imdb_testing'  )  ).read().rstrip(\"\\n\").split(\"\\n\")\n",
    "pred_labels=[ int(label.split()[0] ) for label in pred_labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8844\n",
      "0.8795342943133218\n",
      "0.8902517235850569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "print(accuracy_score(test_labels, pred_labels ))\n",
    "print(precision_score( test_labels, pred_labels ))\n",
    "print(recall_score( test_labels, pred_labels ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode, b64decode\n",
    "import os\n",
    "test_labels=open( os.path.join( '/notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup_binary/test_labels'  )  ).read().rstrip(\"\\n\").split(\"\\n\" )\n",
    "test_data_base64=open( os.path.join( '/notebook/nas-trainings/arne/DGFISMA/DATA/doc_classifier/newsgroup_binary/test_data_base64'  )  ).read().rstrip(\"\\n\").split(\"\\n\" )\n",
    "test_data=[ b64decode( doc ).decode()  for doc in test_data_base64  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: livesey@solntze.wpd.sgi.com (Jon Livesey)\\nSubject: Re: Genocide is Caused by Atheism\\nOrganization: sgi\\nLines: 33\\nDistribution: world\\nNNTP-Posting-Host: solntze.wpd.sgi.com\\n\\nIn article <1r35oe$hqd@horus.ap.mchp.sni.de>, frank@D012S658.uucp (Frank O'Dwyer) writes:\\n|> In article <1r2kt7$6e1@fido.asd.sgi.com> livesey@solntze.wpd.sgi.com (Jon Livesey) writes:\\n|> #In article <1qugin$9tf@horus.ap.mchp.sni.de>, frank@D012S658.uucp (Frank O'Dwyer) writes:\\n|> #|> In article <1qkogg$k@fido.asd.sgi.com> livesey@solntze.wpd.sgi.com (Jon Livesey) writes:\\n|> #|>\\n|> #|> #And in that area, what you care about is whether someone is sceptical,\\n|> #|> #critical and autonomous on the one hand, or gullible, excitable and\\n|> #|> #easily led on the other.\\n|> #|> \\n|> #|> Indeed I may.  And one may be an atheist and also be gullible, excitable\\n|> #|> and easily led.\\n|> #|> \\n|> #|> #I would say that a tendency to worship tyrants and ideologies indicates\\n|> #|> #that a person is easily led.   Whether they have a worship or belief \\n|> #|> #in a supernatural hero rather than an earthly one seems to me to be\\n|> #|> #beside the point.\\n|> #|> \\n|> #|> Sure.  But whether or not they are atheists is what we are discussing,\\n|> #|> not whether they are easily led.  \\n|> #\\n|> #Not if you show that these hypothetical atheists are gullible, excitable\\n|> #and easily led from some concrete cause.   In that case we would also\\n|> #have to discuss if that concrete cause, rather than atheism, was the\\n|> #factor that caused their subsequent behaviour.\\n|> \\n|> I'm not arguing that atheism causes such behaviour - merely that\\n|> it is not relevant to the definition of atheism, which is 'lack of belief in \\n|> gods'.  \\n\\nThrow away the FAQ.   We can all just ask Mr O'Dwyer, since he can\\ndefine the thing that the rest of us only talk about.\\n\\njon.\\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.84\n",
      "0.7899686520376176\n",
      "1\n",
      "0.9099756690997567\n",
      "0.961439588688946\n",
      "2\n",
      "0.9232673267326733\n",
      "0.9467005076142132\n",
      "3\n",
      "0.7983193277310925\n",
      "0.7569721115537849\n"
     ]
    }
   ],
   "source": [
    "#Calculate precision/recall, just for checking if it does what it should do\n",
    "true_labels=open( \"/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/newsgroup/test_labels\" , 'r' ).read().rstrip( \"\\n\" ).split( \"\\n\" )\n",
    "true_labels=[  int(label) for label in true_labels ]\n",
    "\n",
    "pred_labels=open( \"/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/output_folder/results\" , 'r' ).read().rstrip( \"\\n\" ).split( \"\\n\" )\n",
    "pred_labels=[  int(label) for label in pred_labels ]\n",
    "\n",
    "for label_nr in [0,1,2,3]:\n",
    "\n",
    "    true_labels_one_vs_all=[1 if label ==label_nr else 0 for label in true_labels  ]\n",
    "    pred_labels_one_vs_all=[1 if label ==label_nr else 0 for label in pred_labels  ]\n",
    "\n",
    "    print(label_nr)\n",
    "    print(metrics.precision_score(  true_labels_one_vs_all, pred_labels_one_vs_all ))\n",
    "    print(metrics.recall_score(  true_labels_one_vs_all, pred_labels_one_vs_all ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smokers dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "389 documents - 1.687MB (training set)\n",
      "4 categories\n",
      "\n",
      "Using Tfidf Vectorizer.\n",
      "Extracting features via sklearn.feature_selection.SelectKbest using LinearSVC.\n"
     ]
    }
   ],
   "source": [
    "!python ../code_baseline/src/train.py \\\n",
    "--filename /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/smoking/train_data.tsv \\\n",
    "--output_dir  /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/output_folder_smoking \\\n",
    "--vectorizer_type tfidf \\\n",
    "--feature_selection_svc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "101 documents - 0.463MB (test set)\n",
      "4 categories\n",
      "\n",
      "/miniconda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.09      0.15        11\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.80      0.25      0.38        16\n",
      "           3       0.66      0.98      0.79        63\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       101\n",
      "   macro avg       0.49      0.33      0.33       101\n",
      "weighted avg       0.59      0.66      0.57       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../code_baseline/src/test.py \\\n",
    "--filename /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/smoking/test_data.tsv \\\n",
    "--model_path  /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/output_folder_smoking/model.p \\\n",
    "--output_file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/output_folder_smoking/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../code_baseline/src/test.py \\\n",
    "--model_path  /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/output_folder_smoking/model.p \\\n",
    "--output_file /notebook/nas-trainings/arne/OCCAM/text_classification_BERT/output_folder_smoking/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b49c91844b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "x = raw_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.stdin.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.stdin.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c62d58640288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dfdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "raw_input='dfdf'\n",
    "raw_input(\"\").decode(sys.stdin.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters:\n",
    "dirname=\"/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/newsgroup\"\n",
    "output_dir='/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_baseline/output_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_size_mb = size_mb(test_data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "len(test_data), data_test_size_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser.add_argument(\"-v\", \"--verbosity\", type=int, choices=[0, 1, 2],   #==> different choices\n",
    "#                    help=\"increase output verbosity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from base64 import b64encode\n",
    "from base64 import b64decode\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "#categories to consider:\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "remove=()\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "\n",
    "target_names = data_train.target_names\n",
    "\n",
    "print(target_names)\n",
    "\n",
    "dirname=\"/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/newsgroup\"\n",
    "\n",
    "with open( os.path.join(  dirname, \"train_data.tsv\"  ) , 'w'  ) as f:\n",
    "    for doc, label in zip(data_train.data, data_train.target):\n",
    "        encoded_doc = b64encode( doc.encode() )\n",
    "        f.write( f\"{encoded_doc.decode()  }\\t{target_names[label]}\\t{label}\\n\" )\n",
    "        \n",
    "with open( os.path.join(  dirname, \"test_data.tsv\"  ) , 'w'  ) as f:\n",
    "    for doc, label in zip(data_test.data, data_test.target):\n",
    "        encoded_doc = b64encode( doc.encode() )\n",
    "        f.write( f\"{encoded_doc.decode()  }\\t{target_names[label]}\\t{label}\\n\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of actual pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n",
      "\n",
      "Using Tfidf Vectorizer\n"
     ]
    }
   ],
   "source": [
    "#1)Parameters\n",
    "\n",
    "# some parameters:\n",
    "dirname=\"/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/newsgroup\"\n",
    "output_dir='/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/code_baseline/output_folder'\n",
    "\n",
    "# Calculation of features:\n",
    "\n",
    "vectorizer_type='tfidf'  #choices=[ 'tfidf' , 'hashing' ]\n",
    "n_features=2 ** 16  # nr of features when using the hashing vectorizer (ignored when tfidf is used)\n",
    "language='english'  #language used by Vectorizer (i.e. stopwords used).\n",
    "\n",
    "#Feature selection\n",
    "n_select_chi2=None  # An integer number, or None\n",
    "feature_selection_svc=True  #choices=[True, False]\n",
    "penalty_feature_selection='l1'  #penalty of linear SVC classfier used for feature selection\n",
    "\n",
    "# Classifier:\n",
    "\n",
    "penalty='l2'  # Choices=['l1', 'l2', 'elasticnet' ]  penalty of the linear SVC for classification.\n",
    "loss='squared_hinge'  # Choices=['hinge' , 'squared_hinge' ]  hinge or squared_hinge. \n",
    "dual=False  # Solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features (i.e. when doing feature selection beforehand).  \n",
    "\n",
    "\n",
    "#2)Data\n",
    "\n",
    "#create outputdir\n",
    "\n",
    "os.makedirs( output_dir, exist_ok=True  )\n",
    "\n",
    "#read in (train data)\n",
    "data=pd.read_csv(  os.path.join( dirname , \"train_data.tsv\"), sep='\\t' , header=None ) \n",
    "data.shape\n",
    "\n",
    "train_data=data[0].tolist()\n",
    "train_labels=data[2].tolist()\n",
    "del data\n",
    "\n",
    "train_data=[ b64decode( doc ).decode()  for doc in train_data  ]\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(train_data)\n",
    "data_test_size_mb = size_mb(test_data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "    len(train_data), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "    len(test_data), data_test_size_mb))\n",
    "print(\"%d categories\" % len(  np.unique( train_labels  ).tolist()  ))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "#3)Train\n",
    "\n",
    "if vectorizer_type=='tfidf':\n",
    "    print( \"Using Tfidf Vectorizer\" )\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english'  )\n",
    "\n",
    "elif vectorizer_type=='hashing':  \n",
    "    print( \"Using Hashing Vectorizer\" )\n",
    "    vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False,\n",
    "                                   n_features=n_features)\n",
    "\n",
    "if select_chi2:\n",
    "    ch2 = SelectKBest(chi2, k=select_chi2)\n",
    "else:\n",
    "    ch2= None\n",
    "    \n",
    "if feature_selection_svc:\n",
    "    feature_selection=SelectFromModel(LinearSVC(penalty=penalty_feature_selection, dual=False,\n",
    "                                                      tol=1e-3)) \n",
    "else:\n",
    "    feature_selection=None\n",
    "    \n",
    "classifier=LinearSVC(penalty=penalty)\n",
    "    \n",
    "\n",
    "clf=Pipeline([\n",
    "( 'vectorizer', vectorizer)  ,\n",
    "( 'chisquare', ch2  )  ,\n",
    "('feature_selection',  feature_selection  )   ,\n",
    "('classification', LinearSVC(penalty=penalty, loss=loss , dual=dual )  )\n",
    "])\n",
    "\n",
    "\n",
    "clf.fit(  train_data , train_labels )\n",
    "\n",
    "\n",
    "#4) Save the classifier\n",
    "\n",
    "pickle.dump( clf , open( os.path.join( output_dir, \"model.p\"  ), \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82       319\n",
      "           1       0.91      0.96      0.93       389\n",
      "           2       0.93      0.95      0.94       394\n",
      "           3       0.81      0.76      0.78       251\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1353\n",
      "   macro avg       0.87      0.87      0.87      1353\n",
      "weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test on new data:\n",
    "\n",
    "#read in (test data)\n",
    "data=pd.read_csv(  os.path.join( dirname , \"test_data.tsv\"), sep='\\t' , header=None ) \n",
    "data.shape\n",
    "\n",
    "test_data=data[0].tolist()\n",
    "test_labels=data[2].tolist()\n",
    "del data\n",
    "\n",
    "test_data=[ b64decode( doc ).decode()  for doc in test_data  ]\n",
    "\n",
    "#load the classifier \n",
    "\n",
    "clf_load = pickle.load( open(  os.path.join( output_dir, \"model.p\"  ) , \"rb\" ) )\n",
    "\n",
    "pred=clf_load.predict( test_data  )\n",
    "\n",
    "#print(metrics.classification_report(test_labels, pred , target_names=target_names ))\n",
    "print(metrics.classification_report(test_labels, pred  ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tfidf Vectorizer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82       319\n",
      "           1       0.91      0.96      0.93       389\n",
      "           2       0.92      0.95      0.93       394\n",
      "           3       0.80      0.75      0.78       251\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1353\n",
      "   macro avg       0.87      0.86      0.87      1353\n",
      "weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier_type='sgd_svc'  # cho\n",
    "\n",
    "#if vectorizer_type not in [ 'tfidf', 'hashing'  ]:\n",
    "#    raise NameError(  f\"{vectorizer_type} is not in the list of possible vectorizers. Please choose a vectorizer from the following list: 'tfidf' , 'hashing' \"  )\n",
    "\n",
    "#if classifier_type not in [ 'feature_selection_svc', 'sgd_svc', 'sgd_lr']:\n",
    "#    raise NameError(  f\"{classifier_type} is not in the list of possible classifiers. Please choose a classfier from the following list: 'feature_selection_svc' , 'sgd_svc', 'sgd_lr' \"  )\n",
    "    \n",
    "#if penalty not in [ 'l1', 'l2', 'elasticnet' ]:\n",
    "#    raise NameError(  f\"{penalty} is not in the list of possible penalties: 'l1' , 'l2', 'elasticnet' \"  )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Tfidf Vectorizer\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.79      0.81       319\n",
    "           1       0.88      0.96      0.92       389\n",
    "           2       0.93      0.94      0.93       394\n",
    "           3       0.81      0.72      0.76       251\n",
    "\n",
    "   micro avg       0.87      0.87      0.87      1353\n",
    "   macro avg       0.86      0.85      0.86      1353\n",
    "weighted avg       0.87      0.87      0.87      1353\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Tfidf Vectorizer\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.80      0.82       319\n",
    "           1       0.91      0.96      0.93       389\n",
    "           2       0.93      0.95      0.94       394\n",
    "           3       0.81      0.76      0.78       251\n",
    "\n",
    "   micro avg       0.88      0.88      0.88      1353\n",
    "   macro avg       0.87      0.87      0.87      1353\n",
    "weighted avg       0.88      0.88      0.88      1353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8832224685883222"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(  test_labels, pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif classifier_type=='sgd_svc':\n",
    "    clf=Pipeline(\n",
    "    [\n",
    "    ( 'chisquare', None  ),\n",
    "    ( 'vectorizer', vectorizer)  ,\n",
    "    ('classification', SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty , loss='hinge' ,random_state=10 )    )   ]   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.86      0.77      0.81       319\n",
      "     comp.graphics       0.93      0.97      0.95       389\n",
      "         sci.space       0.94      0.96      0.95       394\n",
      "talk.religion.misc       0.75      0.78      0.76       251\n",
      "\n",
      "         micro avg       0.88      0.88      0.88      1353\n",
      "         macro avg       0.87      0.87      0.87      1353\n",
      "      weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred=clf.predict( test_data  )\n",
    "\n",
    "print(metrics.classification_report(test_labels, pred , target_names=target_names ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.86      0.77      0.81       319\n",
      "     comp.graphics       0.93      0.97      0.95       389\n",
      "         sci.space       0.94      0.96      0.95       394\n",
      "talk.religion.misc       0.75      0.78      0.76       251\n",
      "\n",
      "         micro avg       0.88      0.88      0.88      1353\n",
      "         macro avg       0.87      0.87      0.87      1353\n",
      "      weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(  train_data , train_labels )\n",
    "pred=clf.predict( test_data  )\n",
    "\n",
    "print(metrics.classification_report(test_labels, pred , target_names=target_names ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.86      0.78      0.82       319\n",
      "     comp.graphics       0.93      0.97      0.95       389\n",
      "         sci.space       0.94      0.95      0.95       394\n",
      "talk.religion.misc       0.76      0.79      0.78       251\n",
      "\n",
      "         micro avg       0.89      0.89      0.89      1353\n",
      "         macro avg       0.87      0.87      0.87      1353\n",
      "      weighted avg       0.89      0.89      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(  train_data , train_labels )\n",
    "pred=clf.predict( test_data  )\n",
    "\n",
    "print(metrics.classification_report(test_labels, pred , target_names=target_names ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.86      0.78      0.82       319\n",
      "     comp.graphics       0.93      0.97      0.95       389\n",
      "         sci.space       0.95      0.96      0.95       394\n",
      "talk.religion.misc       0.78      0.81      0.79       251\n",
      "\n",
      "         micro avg       0.89      0.89      0.89      1353\n",
      "         macro avg       0.88      0.88      0.88      1353\n",
      "      weighted avg       0.89      0.89      0.89      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(  train_data , train_labels )\n",
    "pred=clf.predict( test_data  )\n",
    "\n",
    "print(metrics.classification_report(test_labels, pred , target_names=target_names ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a sparse vectorizer\n",
      "Using Tfidf Vectorizer\n",
      "done in 0.329135s at 12.091MB/s\n",
      "n_samples: 2034, n_features: 33809\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 0.214905s at 13.343MB/s\n",
      "n_samples: 1353, n_features: 33809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fit tf_idf to train data, then apply tf_idf model on train data, and on the test data. \n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if use_hashing:  #\n",
    "    print( \"Using Hashing Vectorizer\" )\n",
    "    \n",
    "    vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False,\n",
    "                                   n_features=n_features)\n",
    "    X_train = vectorizer.transform(train_data)\n",
    "else:\n",
    "    print( \"Using Tfidf Vectorizer\" )\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    X_train = vectorizer.fit_transform(train_data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(test_data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "if use_hashing:\n",
    "    feature_names = None\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "if select_chi2:\n",
    "    print(\"Extracting %d best features by a chi-squared test\" %\n",
    "          select_chi2)\n",
    "    t0 = time()\n",
    "    ch2 = SelectKBest(chi2, k=select_chi2)\n",
    "    X_train = ch2.fit_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    if feature_names:\n",
    "        # keep selected feature names\n",
    "        feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if classifier_type=='feature_selection_svc':\n",
    "    clf=Pipeline([\n",
    "      ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                                      tol=1e-3))),\n",
    "      ('classification', LinearSVC(penalty=\"l2\"))])\n",
    "    \n",
    "elif classifier_type=='sgd_svc':\n",
    "    #for on-line training\n",
    "    clf=SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty , loss='hinge' )\n",
    "    \n",
    "elif classifier_type=='sgd_lr':\n",
    "    #for on-line training\n",
    "    clf=SGDClassifier(alpha=.0001, max_iter=50, penalty=penalty , loss='log' ) \n",
    "        \n",
    "clf.fit( X_train, y_train )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.87      0.78      0.82       319\n",
      "     comp.graphics       0.92      0.96      0.94       389\n",
      "         sci.space       0.93      0.94      0.94       394\n",
      "talk.religion.misc       0.77      0.79      0.78       251\n",
      "\n",
      "         micro avg       0.88      0.88      0.88      1353\n",
      "         macro avg       0.87      0.87      0.87      1353\n",
      "      weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred=clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report( y_test, pred , target_names=target_names ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.91      0.96      0.93       389\n",
      "talk.religion.misc       0.80      0.76      0.78       251\n",
      "         sci.space       0.93      0.95      0.94       394\n",
      "       alt.atheism       0.84      0.79      0.82       319\n",
      "\n",
      "         micro avg       0.88      0.88      0.88      1353\n",
      "         macro avg       0.87      0.86      0.87      1353\n",
      "      weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred=clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report( y_test, pred , target_names=target_names ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rych@festival.ed.ac.uk (R Hawkes)\n",
      "Subject: 3DS: Where did all the texture rules go?\n",
      "Lines: 21\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "======================================================================\n",
      "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
      "Virtual Environment Laboratory\n",
      "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
      "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( b64decode( encoded_test).decode()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rych@festival.ed.ac.uk (R Hawkes)\n",
      "Subject: 3DS: Where did all the texture rules go?\n",
      "Lines: 21\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "======================================================================\n",
      "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
      "Virtual Environment Laboratory\n",
      "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
      "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( data_train.data[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 ... 3 1 1]\n"
     ]
    }
   ],
   "source": [
    "print( data_test.target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n",
      "\n",
      "Extracting features from the training data using a sparse vectorizer\n",
      "done in 0.257998s at 15.425MB/s\n",
      "n_samples: 2034, n_features: 65536\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 0.182701s at 15.695MB/s\n",
      "n_samples: 1353, n_features: 65536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( clf, X_train, y_train):\n",
    "    clf.fit(X_train, y_train )\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                                  tol=1e-3))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))])\n",
    "\n",
    "trained_clf=train( clf, X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.84      0.77      0.80       319\n",
      "     comp.graphics       0.88      0.96      0.92       389\n",
      "         sci.space       0.94      0.93      0.94       394\n",
      "talk.religion.misc       0.77      0.75      0.76       251\n",
      "\n",
      "         micro avg       0.87      0.87      0.87      1353\n",
      "         macro avg       0.86      0.85      0.85      1353\n",
      "      weighted avg       0.87      0.87      0.87      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred=trained_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report( y_test, pred , target_names=target_names ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.84      0.79      0.81       319\n",
      "     comp.graphics       0.91      0.96      0.94       389\n",
      "         sci.space       0.93      0.95      0.94       394\n",
      "talk.religion.misc       0.79      0.76      0.78       251\n",
      "\n",
      "         micro avg       0.88      0.88      0.88      1353\n",
      "         macro avg       0.87      0.86      0.87      1353\n",
      "      weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred=trained_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report( y_test, pred , target_names=target_names ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 33809)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33809"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "33809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.84      0.79      0.81       319\n",
      "     comp.graphics       0.91      0.96      0.93       389\n",
      "         sci.space       0.93      0.95      0.94       394\n",
      "talk.religion.misc       0.80      0.76      0.78       251\n",
      "\n",
      "         micro avg       0.88      0.88      0.88      1353\n",
      "         macro avg       0.87      0.86      0.87      1353\n",
      "      weighted avg       0.88      0.88      0.88      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred=trained_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report( y_test, pred , target_names=target_names ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[252  11  13  43]\n",
      " [  2 374   9   4]\n",
      " [  2  18 373   1]\n",
      " [ 44   9   8 190]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix( y_test, pred  )   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "metrics.confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8787878787878788"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score( y_test, pred   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', ..., 'l',\n",
       "       '______________________________________________________________________',\n",
       "       'nd'], dtype='<U80')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray( feature_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 33809)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4305c9dbe493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rych@festival.ed.ac.uk (R Hawkes)\n",
      "Subject: 3DS: Where did all the texture rules go?\n",
      "Lines: 21\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "======================================================================\n",
      "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
      "Virtual Environment Laboratory\n",
      "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
      "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( data_train['data'][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print( data_train['target'][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname=\"/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/newsgroup/train_docs\"\n",
    "os.makedirs(  dirname , exist_ok=True )\n",
    "\n",
    "for i, text in enumerate(data_train.data):\n",
    "    with open(  os.path.join( dirname , f'doc{i}' ) , 'w'  ) as f:\n",
    "        f.write( text )\n",
    "        \n",
    "dirname=\"/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/newsgroup/train_labels\"\n",
    "os.makedirs(  dirname , exist_ok=True )\n",
    "        \n",
    "with open(  os.path.join( dirname , f'labels' ) , 'w'  ) as f:\n",
    "    for i, label in enumerate( data_train.target):\n",
    "        f.write(  f\"doc{i}\\t{target_names[label]}\\n\"   )     \n",
    "        \n",
    "dirname=\"/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/newsgroup/test_docs\"\n",
    "os.makedirs(  dirname , exist_ok=True )\n",
    "\n",
    "for i, text in enumerate(data_test.data):\n",
    "    with open(  os.path.join( dirname , f'doc{i}' ) , 'w'  ) as f:\n",
    "        f.write( text )\n",
    "        \n",
    "dirname=\"/notebook/nas-trainings/arne/OCCAM/text_classification_BERT/DATA/newsgroup/test_labels\"\n",
    "os.makedirs(  dirname , exist_ok=True )\n",
    "        \n",
    "with open(  os.path.join( dirname , f'labels' ) , 'w'  ) as f:\n",
    "    for i, label in enumerate( data_test.target):\n",
    "        f.write(  f\"doc{i}\\t{target_names[label]}\\n\"   )     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
